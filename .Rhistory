summarize(count = n()) %>%
top_n(n=10,count) %>%
arrange(desc(count))
cnlp_get_token(anno, include_root = FALSE) %>%
group_bylemmas) %>%
summarize(count = n()) %>%
top_n(n=10,count) %>%
arrange(desc(count))
dep <- cnlp_get_dependency(anno, get_token = TRUE) %>%
filter(relation == "nsubj")
dep$lemma_target %>%
table() %>%
sort(decreasing = TRUE) %>%
head(n = 40)
count_year <- abc %>%
group_by(year) %>%
count() %>%
arrange(desc(n))
count_year
abc[abc$year==2017,]$headline_text
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
# Packages for loading data
library(readxl)
library(httr)
# Packages for manipulating data
library(stringr)
library(lubridate)
# Packages for NLP
library(udpipe)
library(cleanNLP)
# Packages for visualisation
library(igraph)
library(ggraph)
library(ggplot2)
setwd("~/Validly projects")
# Read in the check in data
abc <- read_csv("abcnews-date-text.csv")
# Change the date to a date format
abc$publish_date <- as.Date(as.character(abc$publish_date), format = '%Y%m%d')
# Add new columns for the year, month and day using the lubridate R package
abc <- abc %>%
mutate(year = lubridate::year(abc$publish_date),
month = lubridate::month(abc$publish_date),
day = lubridate::day(abc$publish_date))
# Take a look at the first rows in the  dataset
head(abc)
# Count the number of articles per date
count_year <- abc %>%
group_by(year) %>%
count() %>%
arrange(desc(n))
count_year
# Load the udpipe english pre-trained model
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
# Annotate a subset of the abctext.
abc2017 <- abc[abc$year==2017,]$headline_text
# Remove the abc dataset to save space in memory
rm(abc)
x <- udpipe_annotate(ud_model, x = abc2017, doc_id = seq(1,len(abc2017)))
x <- udpipe_annotate(ud_model, x = abc2017, doc_id = seq(1,length(abc2017)))
freqstats <- txt_freq(x$upos)
ggplot(freqstats, aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("UPOS (Universal Parts of Speech) Frequency of Occurrence") +
labs(fill="Frequency")
x <- as.data.frame(x)
# Let's take a look at the dataframe
str(x)
freqstats <- txt_freq(x$upos)
ggplot(freqstats, aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("UPOS (Universal Parts of Speech) Frequency of Occurrence") +
labs(fill="Frequency")
freqnoun <- txt_freq(subset(x, upos %in% c("NOUN"))$token)
freqnoun %>% filter(freq>75) %>%
ggplot(aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Most Frequent Nouns") +
labs(fill="Frequency")
freqnoun %>% filter(freq>500) %>%
ggplot(aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Most Frequent Nouns") +
labs(fill="Frequency")
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
keywords <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
keywords %>% filter(ngram>1 & freq >20) %>%
ggplot(aes(x=reorder(keyword,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Keywords - Simple Noun Phrases") +
labs(fill="Frequency")
keywords %>% filter(ngram>1 & freq >100) %>%
ggplot(aes(x=reorder(keyword,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Keywords - Simple Noun Phrases") +
labs(fill="Frequency")
# Create a co-occurance dataframe
cooc <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma",
group = c("doc_id"))
wordnetwork <- cooc %>% filter(cooc>20)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "purple") +
geom_node_text(aes(label = name), col = "darkgreen", size = 3) +
theme_graph() +
theme(legend.position = "none") +
labs(title = "Cooccurrences within sentence", subtitle = "Nouns & Adjective")
View(cooc)
wordnetwork <- cooc %>% filter(cooc>75)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "purple") +
geom_node_text(aes(label = name), col = "darkgreen", size = 3) +
theme_graph() +
theme(legend.position = "none") +
labs(title = "Cooccurrences within sentence", subtitle = "Nouns & Adjective")
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "lightgreen") +
geom_node_text(aes(label = name), col = "darkgreen", size = 3) +
theme_graph() +
theme(legend.position = "none") +
labs(title = "Cooccurrences within sentence", subtitle = "Nouns & Adjective")
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
# Packages for loading data
library(readxl)
library(httr)
# Packages for manipulating data
library(stringr)
library(lubridate)
# Packages for NLP
library(udpipe)
library(cleanNLP)
# Packages for visualisation
library(igraph)
library(ggraph)
library(ggplot2)
setwd("~/Validly projects")
# Read in the check in data
abc <- read_csv("abcnews-date-text.csv")
# Change the date to a date format
abc$publish_date <- as.Date(as.character(abc$publish_date), format = '%Y%m%d')
# Add new columns for the year, month and day using the lubridate R package
abc <- abc %>%
mutate(year = lubridate::year(abc$publish_date),
month = lubridate::month(abc$publish_date),
day = lubridate::day(abc$publish_date))
# Take a look at the first rows in the  dataset
head(abc)
# Count the number of articles per date
count_year <- abc %>%
group_by(year) %>%
count() %>%
arrange(desc(n))
count_year
# Load the udpipe english pre-trained model
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
# Annotate a subset of the abctext.
abc2017 <- abc[abc$year==2016,]$headline_text
# Remove the abc dataset to save space in memory
rm(abc)
x <- udpipe_annotate(ud_model, x = abc2017, doc_id = seq(1,length(abc2017)))
x <- as.data.frame(x)
# Let's take a look at the dataframe
str(x)
# Let's take a look at the basic frequency statistics of the upos
freqstats <- txt_freq(x$upos)
ggplot(freqstats, aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("UPOS (Universal Parts of Speech) Frequency of Occurrence") +
labs(fill="Frequency")
#Let's take a look at the basic frequency statistics of the nouns in the upos
freqnoun <- txt_freq(subset(x, upos %in% c("NOUN"))$token)
freqnoun %>% filter(freq>500) %>%
ggplot(aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Most Frequent Nouns") +
labs(fill="Frequency")
# Let's take a look at the basic frequency statistics of the nouns in the upos
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
keywords <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
keywords %>% filter(ngram>1 & freq >100) %>%
ggplot(aes(x=reorder(keyword,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Keywords - Simple Noun Phrases") +
labs(fill="Frequency")
x$id <- unique_identifier(x, fields = c("sentence_id", "doc_id"))
dtm <- subset(x, upos %in% c("NOUN", "ADJ"))
dtm <- document_term_frequencies(dtm, document = "id", term = "lemma")
dtm <- document_term_matrix(dtm)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 5)
termcorrelations <- dtm_cor(dtm)
y <- as_cooccurrence(termcorrelations)
y <- subset(y, term1 < term2 & abs(cooc) > 0.2)
y <- y[order(abs(y$cooc), decreasing = TRUE), ]
head(y,15)
# Save space in memory by removing termcorrelations which is a large dataframe
rm(termcorrelations)
rm(x)
# Create a co-occurance dataframe
cooc <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma",
group = c("doc_id"))
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
# Packages for loading data
library(readxl)
library(httr)
# Packages for manipulating data
library(stringr)
library(lubridate)
# Packages for NLP
library(udpipe)
library(cleanNLP)
# Packages for visualisation
library(igraph)
library(ggraph)
library(ggplot2)
setwd("~/Validly projects")
# Read in the check in data
abc <- read_csv("abcnews-date-text.csv")
# Change the date to a date format
abc$publish_date <- as.Date(as.character(abc$publish_date), format = '%Y%m%d')
# Add new columns for the year, month and day using the lubridate R package
abc <- abc %>%
mutate(year = lubridate::year(abc$publish_date),
month = lubridate::month(abc$publish_date),
day = lubridate::day(abc$publish_date))
# Take a look at the first rows in the  dataset
head(abc)
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
# Packages for loading data
library(readxl)
library(httr)
# Packages for manipulating data
library(stringr)
library(lubridate)
# Packages for NLP
library(udpipe)
library(cleanNLP)
# Packages for visualisation
library(igraph)
library(ggraph)
library(ggplot2)
setwd("~/Validly projects")
# Read in the check in data
abc <- read_csv("abcnews-date-text.csv")
# Change the date to a date format
abc$publish_date <- as.Date(as.character(abc$publish_date), format = '%Y%m%d')
# Add new columns for the year, month and day using the lubridate R package
abc <- abc %>%
mutate(year = lubridate::year(abc$publish_date),
month = lubridate::month(abc$publish_date),
day = lubridate::day(abc$publish_date))
# Take a look at the first rows in the  dataset
head(abc)
# Count the number of articles per date
count_year <- abc %>%
group_by(year) %>%
count() %>%
arrange(desc(year))
count_year
abc2017 <- abc[abc$year==2017,]$headline_text
# Remove the abc dataset to save space in memory
rm(abc)
# Load the udpipe english pre-trained model
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)
# Annotate a subset of the abctext.
x <- udpipe_annotate(ud_model, x = abc2017, doc_id = seq(1,length(abc2017)))
x <- as.data.frame(x)
# Let's take a look at the dataframe
str(x)
# Let's take a look at the basic frequency statistics of the upos
freqstats <- txt_freq(x$upos)
ggplot(freqstats, aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("UPOS (Universal Parts of Speech) Frequency") +
labs(fill="Frequency")
#Let's take a look at the basic frequency statistics of the nouns in the upos
freqnoun <- txt_freq(subset(x, upos %in% c("NOUN"))$token)
freqnoun %>% filter(freq>400) %>%
ggplot(aes(x=reorder(key,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Most Frequent Nouns") +
labs(fill="Frequency")
# Let's take a look at the basic frequency statistics of the nouns in the upos
x$phrase_tag <- as_phrasemachine(x$upos, type = "upos")
keywords <- keywords_phrases(x = x$phrase_tag, term = tolower(x$token),
pattern = "(A|N)*N(P+D*(A|N)*N)*",
is_regex = TRUE, detailed = FALSE)
keywords %>% filter(ngram>1 & freq >100) %>%
ggplot(aes(x=reorder(keyword,freq),y=freq, fill=freq)) +
geom_bar(stat="identity") +
coord_flip() +
xlab("Key") +
ylab("Frequency") +
ggtitle("Keywords - Simple Noun Phrases") +
labs(fill="Frequency")
rm(keywords)
x$id <- unique_identifier(x, fields = c("sentence_id", "doc_id"))
dtm <- subset(x, upos %in% c("NOUN", "ADJ"))
dtm <- document_term_frequencies(dtm, document = "id", term = "lemma")
dtm <- document_term_matrix(dtm)
dtm <- dtm_remove_lowfreq(dtm, minfreq = 5)
termcorrelations <- dtm_cor(dtm)
y <- as_cooccurrence(termcorrelations)
y <- subset(y, term1 < term2 & abs(cooc) > 0.2)
y <- y[order(abs(y$cooc), decreasing = TRUE), ]
head(y,15)
# Save space in memory by removing termcorrelations which is a large dataframe
rm(termcorrelations)
rm(dtm)
rm(y)
# Create a co-occurance dataframe
cooc <- cooccurrence(x = subset(x, upos %in% c("NOUN", "ADJ")),
term = "lemma",
group = c("doc_id"))
# Create a co-occurance dataframe
cooc1 <- cooccurrence(x = subset(x, upos %in% c("NOUN", "VERB")),
term = "lemma",
group = c("doc_id"))
wordnetwork <- cooc %>% filter(cooc>30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "green") +
geom_node_text(aes(label = name), col = "darkblue", size = 3) +
theme_graph() +
theme(legend.position = "none") +
labs(title = "Co - occurrences within sentence", subtitle = "Nouns & Adjective")
wordnetwork <- cooc1 %>% filter(cooc>30)
wordnetwork <- graph_from_data_frame(wordnetwork)
ggraph(wordnetwork, layout = "fr") +
geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = "green") +
geom_node_text(aes(label = name), col = "darkblue", size = 3) +
theme_graph() +
theme(legend.position = "none") +
labs(title = "Co - occurrences within sentence", subtitle = "Nouns & Verbs")
knitr::opts_chunk$set(echo = FALSE,
dev = "svglite",
fig.ext = ".svg"
)
knitr::knit_engines$set(python = reticulate::eng_python)
library(tidyverse)
# Packages for manipulating data
library(stringr)
library(lubridate)
# Packages for NLP
library(udpipe)
library(NLP)
library(openNLP)
#install.packages("openNLPmodels.en",
#                 repos = "http://datacube.wu.ac.at/",
#                 type = "source")
library(cleanNLP)
# cnlp_download_corenlp() # install the coreNLP Java back end 'CoreNLP' <http://stanfordnlp.github.io/CoreNLP/>
# Packages for Python interface
library(reticulate)
use_virtualenv("r-reticulate")
# Packages for visualisation
library(magick)
library(rsvg)
magick_config()$rsvg
library(Cairo)
capabilities()["cairo"]
# First set the executable. Note this needs to be set before any initialising
use_python("C:/Users/HOME/Anaconda3/python.exe")
# py_available(initialize = TRUE) # should give TRUE
# Check Python configuration
py_config()
# Initialise the spaCy backend
cnlp_init_spacy()
setwd("~/Validly projects")
# Read in the check in data
abc <- read_csv("abcnews-date-text.csv")
# Change the date to a date format
abc$publish_date <- as.Date(as.character(abc$publish_date), format = '%Y%m%d')
# Add new columns for the year, month and day using the lubridate R package
abc <- abc %>%
mutate(year = lubridate::year(abc$publish_date),
month = lubridate::month(abc$publish_date),
day = lubridate::day(abc$publish_date))
# Take a look at the first rows in the  dataset
head(abc)
# Count the number of articles per year
count_year <- abc %>%
group_by(year) %>%
count() %>%
arrange(desc(year))
count_year
# Count the number of articles per month
count_year_month <- abc %>%
filter(year==2017) %>%
group_by(year,month) %>%
count() %>%
arrange(desc(month))
count_year_month
abc2017 <- abc[abc$year==2017 & abc$month==1,]$headline_text
# Remove the abc dataset to save space in memory
rm(abc)
# Collapse the list into one string
abctext <- paste(abc2017, collapse = " ")
abctext <- as.String(abctext)
writeLines(abctext, tf <- tempfile())
# Annotate the string tf by running the annotation engine over the corpus of text
anno <- cnlp_annotate(tf)
# Summarise the tokens by parts of speech
cnlp_get_token(anno, include_root = FALSE) %>%
group_by(upos) %>%
summarize(posnum = n()) %>%
arrange(desc(posnum))
# Summarise the count of entities
cnlp_get_entity(anno) %>%
group_by(entity_type) %>%
summarize(count = n())  %>%
arrange(desc(count))
# Extract the entities of type GPE which is are geo-political entities such as city, state/province, and country
cnlp_get_entity(anno) %>%
filter(entity_type == "GPE") %>%
group_by(entity) %>%
summarize(count = n()) %>%
arrange(desc(count))
# Extract the entities of type NORP which are Nationalities or religious or political groups.
cnlp_get_entity(anno) %>%
filter(entity_type == "NORP") %>%
group_by(entity) %>%
summarize(count = n()) %>%
arrange(desc(count))
# Extract the entities of type PERSON which are People, including fictional.
cnlp_get_entity(anno) %>%
filter(entity_type == "PERSON") %>%
group_by(entity) %>%
summarize(count = n()) %>%
arrange(desc(count))
# Extract the entities of type ORG which are Companies, agencies, institutions, etc.
cnlp_get_entity(anno) %>%
filter(entity_type == "ORG") %>%
group_by(entity) %>%
summarize(count = n()) %>%
arrange(desc(count))
# Extract the entities of type MONEY which are Monetary values, including unit.
cnlp_get_entity(anno) %>%
filter(entity_type == "MONEY") %>%
group_by(entity) %>%
summarize(count = n()) %>%
arrange(desc(count))
py_available(initialize = TRUE)
viz <- image_read_svg('~/Validly projects/sentence.svg',width=400)
viz <- image_read_svg('~/sentence.svg',width=400)
library(Rsvg)
library(rsvg)
library(grimport2)
library(grImport2)
viz <- image_read_svg('~/Validly projects/sentence.svg',width=400)
viz <- image_read_svg('~/Validly projects/sentence.csv',width=400)
viz <- image_read('~/Validly projects/sentence.csv',width=400)
viz <- image_read('~/Validly projects/sentence.csv'0)
viz <- image_read('~/Validly projects/sentence.csv')
viz <- read_csv('~/Validly projects/sentence.csv')
print(viz)
View(viz)
svg(viz)
svg("sentence.csv")
dev.off()
# Ongoing
# Load the blogdown package and run the install_hugo() command. This can just be done in the console.
# Load blogdown package and install Hugo
library(blogdown)
setwd("~/blogdown_source")
build_site()
